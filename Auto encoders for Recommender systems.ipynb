{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the ratings data\n",
    "ratings = pd.read_csv('/Users/ratings.dat',sep=\"::\", header = None, engine='python')\n",
    "# Lets pivot the data to get it at a user level\n",
    "ratings_pivot = pd.pivot_table(ratings[[0,1,2]],\\\n",
    "          values=2, index=0, columns=1 ).fillna(0)\n",
    "# creating train and test sets\n",
    "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading the ratings data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total Movies=3706\n",
    "total users=6040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding how many nodes wach layers should have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_inpl = 3706  \n",
    "n_nodes_hl1  = 256  \n",
    "n_nodes_outl = 3706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first hidden layer has 784*32 weights and 32biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1,n_nodes_hl1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1,n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pavantej/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# user with 3706 ratings goes in\n",
    "input_layer = tf.placeholder('float', [None, 3706])\n",
    "# add a constant node to the first layer\n",
    "# it needs to have the same shape as the input layer for me to be\n",
    "# able to concatinate it later\n",
    "input_layer_const = tf.fill( [tf.shape(input_layer)[0], 1] ,1.0  )\n",
    "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
    "# multiply output of input_layer wth a weight matrix \n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,\\\n",
    "hidden_1_layer_vals['weights']))\n",
    "# adding one bias node to the hidden layer\n",
    "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1] ,1.0  )\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
    "# multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = tf.matmul( layer_concat,output_layer_vals['weights'])\n",
    "# output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, 3706])\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 100  # how many images to use together for training\n",
    "hm_epochs =200    # how many times to go through the entire dataset\n",
    "tot_users = X_train.shape[0] # total number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 57.88626340072136 MSE test 57.94027863080741\n",
      "Epoch 0 / 200 loss: 3957.451332092285\n",
      "MSE train 37.77522261951648 MSE test 38.134431381539535\n",
      "Epoch 1 / 200 loss: 2254.3705291748047\n",
      "MSE train 27.27880629737198 MSE test 27.719474286424404\n",
      "Epoch 2 / 200 loss: 1551.8767681121826\n",
      "MSE train 21.26687733091505 MSE test 21.640621916731813\n",
      "Epoch 3 / 200 loss: 1163.6865863800049\n",
      "MSE train 17.669576071853662 MSE test 18.027455347650772\n",
      "Epoch 4 / 200 loss: 937.7448234558105\n",
      "MSE train 15.161060882412565 MSE test 15.480147369725376\n",
      "Epoch 5 / 200 loss: 793.4605398178101\n",
      "MSE train 13.386333222528062 MSE test 13.740983857021979\n",
      "Epoch 6 / 200 loss: 690.7237911224365\n",
      "MSE train 12.040419170060671 MSE test 12.417602912813644\n",
      "Epoch 7 / 200 loss: 615.6172866821289\n",
      "MSE train 11.01703084928201 MSE test 11.370798700791303\n",
      "Epoch 8 / 200 loss: 558.3523693084717\n",
      "MSE train 10.189855005894207 MSE test 10.532173031200141\n",
      "Epoch 9 / 200 loss: 513.5820817947388\n",
      "MSE train 9.507384614503682 MSE test 9.85373832104807\n",
      "Epoch 10 / 200 loss: 477.1660318374634\n",
      "MSE train 8.930846965680416 MSE test 9.288084486209316\n",
      "Epoch 11 / 200 loss: 446.5465884208679\n",
      "MSE train 8.44008005545458 MSE test 8.805500244082417\n",
      "Epoch 12 / 200 loss: 420.6801166534424\n",
      "MSE train 8.017094411033435 MSE test 8.394090186692594\n",
      "Epoch 13 / 200 loss: 398.44468784332275\n",
      "MSE train 7.639760025807287 MSE test 8.034620588461994\n",
      "Epoch 14 / 200 loss: 378.9909210205078\n",
      "MSE train 7.306992661006997 MSE test 7.716029730331155\n",
      "Epoch 15 / 200 loss: 361.77291679382324\n",
      "MSE train 7.011293902823651 MSE test 7.426797682394375\n",
      "Epoch 16 / 200 loss: 346.4356985092163\n",
      "MSE train 6.752752886255966 MSE test 7.174558382723582\n",
      "Epoch 17 / 200 loss: 332.9592728614807\n",
      "MSE train 6.514072540229037 MSE test 6.943058753525102\n",
      "Epoch 18 / 200 loss: 320.96198654174805\n",
      "MSE train 6.297145037899969 MSE test 6.732962176854285\n",
      "Epoch 19 / 200 loss: 309.8547172546387\n",
      "MSE train 6.0950503974449255 MSE test 6.539164295033697\n",
      "Epoch 20 / 200 loss: 299.7677435874939\n",
      "MSE train 5.907966378880294 MSE test 6.358988925601441\n",
      "Epoch 21 / 200 loss: 290.2954511642456\n",
      "MSE train 5.740260241207654 MSE test 6.195175678964664\n",
      "Epoch 22 / 200 loss: 281.64282178878784\n",
      "MSE train 5.5875889811054185 MSE test 6.044557181015343\n",
      "Epoch 23 / 200 loss: 273.8558440208435\n",
      "MSE train 5.447125844218446 MSE test 5.904969441419616\n",
      "Epoch 24 / 200 loss: 266.7274503707886\n",
      "MSE train 5.316625132346033 MSE test 5.775222942330148\n",
      "Epoch 25 / 200 loss: 260.1706585884094\n",
      "MSE train 5.193943477971278 MSE test 5.655391549715108\n",
      "Epoch 26 / 200 loss: 254.02091932296753\n",
      "MSE train 5.078929406885243 MSE test 5.544473184824428\n",
      "Epoch 27 / 200 loss: 248.25772619247437\n",
      "MSE train 4.9692402110315115 MSE test 5.441121714270613\n",
      "Epoch 28 / 200 loss: 242.8250756263733\n",
      "MSE train 4.8652474109048365 MSE test 5.344521858457757\n",
      "Epoch 29 / 200 loss: 237.64029550552368\n",
      "MSE train 4.770799325731606 MSE test 5.254518813251453\n",
      "Epoch 30 / 200 loss: 232.77698731422424\n",
      "MSE train 4.679139026150641 MSE test 5.168095964071627\n",
      "Epoch 31 / 200 loss: 228.31263542175293\n",
      "MSE train 4.590964060403244 MSE test 5.085551787029384\n",
      "Epoch 32 / 200 loss: 223.9734456539154\n",
      "MSE train 4.507719194071858 MSE test 5.007741024697604\n",
      "Epoch 33 / 200 loss: 219.80790257453918\n",
      "MSE train 4.43024641597376 MSE test 4.934917308717945\n",
      "Epoch 34 / 200 loss: 215.8822648525238\n",
      "MSE train 4.357188488465859 MSE test 4.865783270367773\n",
      "Epoch 35 / 200 loss: 212.22260427474976\n",
      "MSE train 4.2853357912569106 MSE test 4.798580766002428\n",
      "Epoch 36 / 200 loss: 208.74369144439697\n",
      "MSE train 4.216918623976344 MSE test 4.733441625335786\n",
      "Epoch 37 / 200 loss: 205.33059430122375\n",
      "MSE train 4.153002411409609 MSE test 4.671704338511594\n",
      "Epoch 38 / 200 loss: 202.1052267551422\n",
      "MSE train 4.09351285531893 MSE test 4.613319207277571\n",
      "Epoch 39 / 200 loss: 199.08910846710205\n",
      "MSE train 4.03615666840558 MSE test 4.55713045157693\n",
      "Epoch 40 / 200 loss: 196.26318907737732\n",
      "MSE train 3.9807399238375165 MSE test 4.502967196304317\n",
      "Epoch 41 / 200 loss: 193.5251021385193\n",
      "MSE train 3.927403439486019 MSE test 4.4505144971363\n",
      "Epoch 42 / 200 loss: 190.89414191246033\n",
      "MSE train 3.875984753871984 MSE test 4.399655710585451\n",
      "Epoch 43 / 200 loss: 188.35830521583557\n",
      "MSE train 3.8265183101379443 MSE test 4.350442051500075\n",
      "Epoch 44 / 200 loss: 185.90768361091614\n",
      "MSE train 3.7790367390774393 MSE test 4.303311125377954\n",
      "Epoch 45 / 200 loss: 183.5550138950348\n",
      "MSE train 3.7321608114485114 MSE test 4.25734559507082\n",
      "Epoch 46 / 200 loss: 181.28438115119934\n",
      "MSE train 3.68671778453919 MSE test 4.212960731141395\n",
      "Epoch 47 / 200 loss: 179.0514097213745\n",
      "MSE train 3.641989221795443 MSE test 4.1697949893534\n",
      "Epoch 48 / 200 loss: 176.8783574104309\n",
      "MSE train 3.598359992244438 MSE test 4.1279898551649445\n",
      "Epoch 49 / 200 loss: 174.7399024963379\n",
      "MSE train 3.5560871888965813 MSE test 4.087747954714076\n",
      "Epoch 50 / 200 loss: 172.6588122844696\n",
      "MSE train 3.5146805862786525 MSE test 4.048885899916182\n",
      "Epoch 51 / 200 loss: 170.6407561302185\n",
      "MSE train 3.4738941940644836 MSE test 4.0109095264591295\n",
      "Epoch 52 / 200 loss: 168.66077876091003\n",
      "MSE train 3.4351590007540755 MSE test 3.9741818789395085\n",
      "Epoch 53 / 200 loss: 166.71585130691528\n",
      "MSE train 3.3981681184038686 MSE test 3.9386875199449056\n",
      "Epoch 54 / 200 loss: 164.86955666542053\n",
      "MSE train 3.362777289709866 MSE test 3.9042816412430272\n",
      "Epoch 55 / 200 loss: 163.10627603530884\n",
      "MSE train 3.328708088647674 MSE test 3.8709901545957432\n",
      "Epoch 56 / 200 loss: 161.42145681381226\n",
      "MSE train 3.2953241813267913 MSE test 3.8384247545873027\n",
      "Epoch 57 / 200 loss: 159.79252648353577\n",
      "MSE train 3.262445038694105 MSE test 3.8066244620339407\n",
      "Epoch 58 / 200 loss: 158.19315361976624\n",
      "MSE train 3.2308147299025314 MSE test 3.7756296491434855\n",
      "Epoch 59 / 200 loss: 156.6187448501587\n",
      "MSE train 3.2004440178976505 MSE test 3.745841094357654\n",
      "Epoch 60 / 200 loss: 155.11250519752502\n",
      "MSE train 3.170654685733775 MSE test 3.71658674734817\n",
      "Epoch 61 / 200 loss: 153.65901231765747\n",
      "MSE train 3.141726687657337 MSE test 3.6875249203498517\n",
      "Epoch 62 / 200 loss: 152.2323043346405\n",
      "MSE train 3.1136090866617643 MSE test 3.659133727116759\n",
      "Epoch 63 / 200 loss: 150.8497667312622\n",
      "MSE train 3.0855281416941 MSE test 3.6312778917359627\n",
      "Epoch 64 / 200 loss: 149.5009937286377\n",
      "MSE train 3.0582667931585705 MSE test 3.604288532729534\n",
      "Epoch 65 / 200 loss: 148.154953956604\n",
      "MSE train 3.0318138458757353 MSE test 3.5782135486730335\n",
      "Epoch 66 / 200 loss: 146.85102343559265\n",
      "MSE train 3.006052905074314 MSE test 3.5528629984102404\n",
      "Epoch 67 / 200 loss: 145.5872278213501\n",
      "MSE train 2.981266234058106 MSE test 3.5283313585005955\n",
      "Epoch 68 / 200 loss: 144.35378527641296\n",
      "MSE train 2.957589511121126 MSE test 3.5046112934359024\n",
      "Epoch 69 / 200 loss: 143.17352890968323\n",
      "MSE train 2.934181895151423 MSE test 3.481287819645473\n",
      "Epoch 70 / 200 loss: 142.03969836235046\n",
      "MSE train 2.91090439478255 MSE test 3.458512754086814\n",
      "Epoch 71 / 200 loss: 140.9187717437744\n",
      "MSE train 2.887429281174809 MSE test 3.436278134660498\n",
      "Epoch 72 / 200 loss: 139.79919743537903\n",
      "MSE train 2.8640197014949527 MSE test 3.4146563133224284\n",
      "Epoch 73 / 200 loss: 138.67187309265137\n",
      "MSE train 2.8413805120165514 MSE test 3.393608086875067\n",
      "Epoch 74 / 200 loss: 137.54931664466858\n",
      "MSE train 2.81895237599079 MSE test 3.372915096189931\n",
      "Epoch 75 / 200 loss: 136.4655568599701\n",
      "MSE train 2.797114436061231 MSE test 3.3526645369340566\n",
      "Epoch 76 / 200 loss: 135.3914737701416\n",
      "MSE train 2.775781806283114 MSE test 3.332927598135257\n",
      "Epoch 77 / 200 loss: 134.34651803970337\n",
      "MSE train 2.7547748689856615 MSE test 3.313586929123537\n",
      "Epoch 78 / 200 loss: 133.32375073432922\n",
      "MSE train 2.7344210317842372 MSE test 3.294703786963224\n",
      "Epoch 79 / 200 loss: 132.32054090499878\n",
      "MSE train 2.7146510427705715 MSE test 3.2762785018916425\n",
      "Epoch 80 / 200 loss: 131.34903848171234\n",
      "MSE train 2.695592561902764 MSE test 3.2582260231398656\n",
      "Epoch 81 / 200 loss: 130.40514385700226\n",
      "MSE train 2.676758492617509 MSE test 3.240395766520052\n",
      "Epoch 82 / 200 loss: 129.49138760566711\n",
      "MSE train 2.6581872411878913 MSE test 3.223028750315201\n",
      "Epoch 83 / 200 loss: 128.5887600183487\n",
      "MSE train 2.640338113727352 MSE test 3.2063354273533387\n",
      "Epoch 84 / 200 loss: 127.69917178153992\n",
      "MSE train 2.623205997907358 MSE test 3.1901983811788766\n",
      "Epoch 85 / 200 loss: 126.84716665744781\n",
      "MSE train 2.6070276858689914 MSE test 3.1743658889196964\n",
      "Epoch 86 / 200 loss: 126.02980411052704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 2.591306635922387 MSE test 3.1589901850537414\n",
      "Epoch 87 / 200 loss: 125.25628542900085\n",
      "MSE train 2.575617562073791 MSE test 3.1438285436053346\n",
      "Epoch 88 / 200 loss: 124.50252830982208\n",
      "MSE train 2.559846271901319 MSE test 3.1288325407702016\n",
      "Epoch 89 / 200 loss: 123.74914145469666\n",
      "MSE train 2.544404071127683 MSE test 3.1141457157181125\n",
      "Epoch 90 / 200 loss: 122.99239933490753\n",
      "MSE train 2.52914622202631 MSE test 3.099733408471619\n",
      "Epoch 91 / 200 loss: 122.25191795825958\n",
      "MSE train 2.5138997054926135 MSE test 3.085508793431884\n",
      "Epoch 92 / 200 loss: 121.51937675476074\n",
      "MSE train 2.4993035737644362 MSE test 3.0715464229852154\n",
      "Epoch 93 / 200 loss: 120.79351449012756\n",
      "MSE train 2.4850352314665765 MSE test 3.0578933943012707\n",
      "Epoch 94 / 200 loss: 120.09545278549194\n",
      "MSE train 2.471020426343034 MSE test 3.044493724296292\n",
      "Epoch 95 / 200 loss: 119.4123649597168\n",
      "MSE train 2.4571940362266287 MSE test 3.0312779713434135\n",
      "Epoch 96 / 200 loss: 118.74200427532196\n",
      "MSE train 2.443704123850126 MSE test 3.0183504065928286\n",
      "Epoch 97 / 200 loss: 118.08133041858673\n",
      "MSE train 2.43021359286327 MSE test 3.005621688964632\n",
      "Epoch 98 / 200 loss: 117.43550956249237\n",
      "MSE train 2.4168549483915855 MSE test 2.993093213925798\n",
      "Epoch 99 / 200 loss: 116.7898120880127\n",
      "MSE train 2.403990031919113 MSE test 2.9809481607594464\n",
      "Epoch 100 / 200 loss: 116.15172672271729\n",
      "MSE train 2.391475340735162 MSE test 2.9691567625910484\n",
      "Epoch 101 / 200 loss: 115.5356833934784\n",
      "MSE train 2.37899347333163 MSE test 2.9575429153463433\n",
      "Epoch 102 / 200 loss: 114.93460059165955\n",
      "MSE train 2.3665919137714027 MSE test 2.946050931506131\n",
      "Epoch 103 / 200 loss: 114.33409976959229\n",
      "MSE train 2.3545109633394654 MSE test 2.934649694756347\n",
      "Epoch 104 / 200 loss: 113.74119853973389\n",
      "MSE train 2.3428037046277135 MSE test 2.9235490869684737\n",
      "Epoch 105 / 200 loss: 113.16202807426453\n",
      "MSE train 2.3313949561133915 MSE test 2.912666798144758\n",
      "Epoch 106 / 200 loss: 112.60153412818909\n",
      "MSE train 2.32024145242865 MSE test 2.9020709678240646\n",
      "Epoch 107 / 200 loss: 112.05450689792633\n",
      "MSE train 2.3091724237739175 MSE test 2.891655018306867\n",
      "Epoch 108 / 200 loss: 111.51938223838806\n",
      "MSE train 2.298501037543579 MSE test 2.881383046169879\n",
      "Epoch 109 / 200 loss: 110.98852229118347\n",
      "MSE train 2.2878025822324464 MSE test 2.8711588456188877\n",
      "Epoch 110 / 200 loss: 110.47542941570282\n",
      "MSE train 2.2770404376152378 MSE test 2.860827748318135\n",
      "Epoch 111 / 200 loss: 109.96006524562836\n",
      "MSE train 2.2662326827920256 MSE test 2.850627704110531\n",
      "Epoch 112 / 200 loss: 109.44308483600616\n",
      "MSE train 2.2556433953718034 MSE test 2.840814294674171\n",
      "Epoch 113 / 200 loss: 108.92524075508118\n",
      "MSE train 2.244948597465229 MSE test 2.8311873100101534\n",
      "Epoch 114 / 200 loss: 108.41572499275208\n",
      "MSE train 2.2342824687039338 MSE test 2.8216746712991574\n",
      "Epoch 115 / 200 loss: 107.90167343616486\n",
      "MSE train 2.2242738659064396 MSE test 2.8124128364220398\n",
      "Epoch 116 / 200 loss: 107.39029717445374\n",
      "MSE train 2.214510168990718 MSE test 2.803316476098078\n",
      "Epoch 117 / 200 loss: 106.91126692295074\n",
      "MSE train 2.2049157884954247 MSE test 2.7943236619300165\n",
      "Epoch 118 / 200 loss: 106.44265604019165\n",
      "MSE train 2.195859655350399 MSE test 2.7855893607601\n",
      "Epoch 119 / 200 loss: 105.98350596427917\n",
      "MSE train 2.187072715333575 MSE test 2.7770624418544507\n",
      "Epoch 120 / 200 loss: 105.55089461803436\n",
      "MSE train 2.17851759637291 MSE test 2.768711822524432\n",
      "Epoch 121 / 200 loss: 105.12952387332916\n",
      "MSE train 2.1703808669718168 MSE test 2.760549558515623\n",
      "Epoch 122 / 200 loss: 104.71935975551605\n",
      "MSE train 2.1622839008723176 MSE test 2.7524897998846014\n",
      "Epoch 123 / 200 loss: 104.32863855361938\n",
      "MSE train 2.1539157107440112 MSE test 2.744461742203874\n",
      "Epoch 124 / 200 loss: 103.93865692615509\n",
      "MSE train 2.1455623159495154 MSE test 2.7364912530871566\n",
      "Epoch 125 / 200 loss: 103.53569650650024\n",
      "MSE train 2.1371758227541706 MSE test 2.728579725087364\n",
      "Epoch 126 / 200 loss: 103.13421368598938\n",
      "MSE train 2.1285665282685318 MSE test 2.7207054498715677\n",
      "Epoch 127 / 200 loss: 102.73059964179993\n",
      "MSE train 2.1200218725709274 MSE test 2.7129538269296942\n",
      "Epoch 128 / 200 loss: 102.3173553943634\n",
      "MSE train 2.111846560633607 MSE test 2.705307723166248\n",
      "Epoch 129 / 200 loss: 101.90764474868774\n",
      "MSE train 2.1037385550590386 MSE test 2.6977737637489785\n",
      "Epoch 130 / 200 loss: 101.51533722877502\n",
      "MSE train 2.0956497919423183 MSE test 2.6902771710458664\n",
      "Epoch 131 / 200 loss: 101.12596392631531\n",
      "MSE train 2.0877304018174203 MSE test 2.6828321898606267\n",
      "Epoch 132 / 200 loss: 100.73743104934692\n",
      "MSE train 2.079969157550498 MSE test 2.675626108568487\n",
      "Epoch 133 / 200 loss: 100.35694551467896\n",
      "MSE train 2.072187322090932 MSE test 2.668551823084836\n",
      "Epoch 134 / 200 loss: 99.98348701000214\n",
      "MSE train 2.064701610472986 MSE test 2.6616324770318767\n",
      "Epoch 135 / 200 loss: 99.6093282699585\n",
      "MSE train 2.0574885724447776 MSE test 2.6548625337664284\n",
      "Epoch 136 / 200 loss: 99.25035166740417\n",
      "MSE train 2.050438728721571 MSE test 2.6481806296619226\n",
      "Epoch 137 / 200 loss: 98.90444028377533\n",
      "MSE train 2.0435443540150526 MSE test 2.641588780146768\n",
      "Epoch 138 / 200 loss: 98.5662270784378\n",
      "MSE train 2.0367163379628868 MSE test 2.6351458204258633\n",
      "Epoch 139 / 200 loss: 98.23494613170624\n",
      "MSE train 2.0299633688164818 MSE test 2.628916639552045\n",
      "Epoch 140 / 200 loss: 97.90762531757355\n",
      "MSE train 2.023217059997938 MSE test 2.622799942101668\n",
      "Epoch 141 / 200 loss: 97.58355796337128\n",
      "MSE train 2.016680775292084 MSE test 2.6168123834890524\n",
      "Epoch 142 / 200 loss: 97.25966942310333\n",
      "MSE train 2.010545558812971 MSE test 2.61097468991943\n",
      "Epoch 143 / 200 loss: 96.94638586044312\n",
      "MSE train 2.004565525514226 MSE test 2.6052110723250173\n",
      "Epoch 144 / 200 loss: 96.6524109840393\n",
      "MSE train 1.998786625954449 MSE test 2.5995299940229293\n",
      "Epoch 145 / 200 loss: 96.36587202548981\n",
      "MSE train 1.9930859363060052 MSE test 2.593934891719098\n",
      "Epoch 146 / 200 loss: 96.08862829208374\n",
      "MSE train 1.987351377832916 MSE test 2.5883648779040525\n",
      "Epoch 147 / 200 loss: 95.81466603279114\n",
      "MSE train 1.9813756417014752 MSE test 2.5827457691810434\n",
      "Epoch 148 / 200 loss: 95.53924810886383\n",
      "MSE train 1.975154894237441 MSE test 2.5770224865541995\n",
      "Epoch 149 / 200 loss: 95.2517637014389\n",
      "MSE train 1.969046721168798 MSE test 2.5713454718776885\n",
      "Epoch 150 / 200 loss: 94.95268070697784\n",
      "MSE train 1.9630701736708471 MSE test 2.565734427770724\n",
      "Epoch 151 / 200 loss: 94.65863382816315\n",
      "MSE train 1.957047845818398 MSE test 2.5602907796918233\n",
      "Epoch 152 / 200 loss: 94.36859917640686\n",
      "MSE train 1.9513645296823292 MSE test 2.5550846535105203\n",
      "Epoch 153 / 200 loss: 94.08075892925262\n",
      "MSE train 1.9458457036161811 MSE test 2.5500265153309365\n",
      "Epoch 154 / 200 loss: 93.80879783630371\n",
      "MSE train 1.9402384563439443 MSE test 2.5450307641465972\n",
      "Epoch 155 / 200 loss: 93.54346477985382\n",
      "MSE train 1.9348093241367268 MSE test 2.5400957754845033\n",
      "Epoch 156 / 200 loss: 93.27483212947845\n",
      "MSE train 1.9295801212185424 MSE test 2.535214165853902\n",
      "Epoch 157 / 200 loss: 93.01467728614807\n",
      "MSE train 1.9242459796102545 MSE test 2.530382293699735\n",
      "Epoch 158 / 200 loss: 92.76340985298157\n",
      "MSE train 1.9189252591801766 MSE test 2.525583549379782\n",
      "Epoch 159 / 200 loss: 92.50694441795349\n",
      "MSE train 1.914016925304345 MSE test 2.520871873988045\n",
      "Epoch 160 / 200 loss: 92.25211310386658\n",
      "MSE train 1.9092845768693902 MSE test 2.516294031031707\n",
      "Epoch 161 / 200 loss: 92.01726746559143\n",
      "MSE train 1.9047036859818822 MSE test 2.511831852933531\n",
      "Epoch 162 / 200 loss: 91.7901816368103\n",
      "MSE train 1.9001957449059022 MSE test 2.507444406277227\n",
      "Epoch 163 / 200 loss: 91.57036173343658\n",
      "MSE train 1.8955510521406078 MSE test 2.503038466388757\n",
      "Epoch 164 / 200 loss: 91.35358595848083\n",
      "MSE train 1.8909033409542055 MSE test 2.4986248516164635\n",
      "Epoch 165 / 200 loss: 91.13045072555542\n",
      "MSE train 1.8861723230509075 MSE test 2.4942314137834423\n",
      "Epoch 166 / 200 loss: 90.90749144554138\n",
      "MSE train 1.8813067713733966 MSE test 2.489794059243868\n",
      "Epoch 167 / 200 loss: 90.6799750328064\n",
      "MSE train 1.87647057828502 MSE test 2.485376008948222\n",
      "Epoch 168 / 200 loss: 90.44569182395935\n",
      "MSE train 1.8718147042239897 MSE test 2.4810775532617275\n",
      "Epoch 169 / 200 loss: 90.2135249376297\n",
      "MSE train 1.867230600271287 MSE test 2.476845858606104\n",
      "Epoch 170 / 200 loss: 89.99007499217987\n",
      "MSE train 1.862611801605976 MSE test 2.472666731546325\n",
      "Epoch 171 / 200 loss: 89.76963067054749\n",
      "MSE train 1.857845315928884 MSE test 2.4685094996377894\n",
      "Epoch 172 / 200 loss: 89.54723966121674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 1.8530821206894628 MSE test 2.4643343184355144\n",
      "Epoch 173 / 200 loss: 89.31762218475342\n",
      "MSE train 1.8484304637345428 MSE test 2.460201413739533\n",
      "Epoch 174 / 200 loss: 89.08764851093292\n",
      "MSE train 1.8439362706535805 MSE test 2.456101228501545\n",
      "Epoch 175 / 200 loss: 88.86523675918579\n",
      "MSE train 1.8396000468061702 MSE test 2.452066732285719\n",
      "Epoch 176 / 200 loss: 88.64937567710876\n",
      "MSE train 1.8352613146281445 MSE test 2.44805656919894\n",
      "Epoch 177 / 200 loss: 88.44127416610718\n",
      "MSE train 1.8308559839678813 MSE test 2.4440144782740805\n",
      "Epoch 178 / 200 loss: 88.232905626297\n",
      "MSE train 1.8266749448653232 MSE test 2.439992546482714\n",
      "Epoch 179 / 200 loss: 88.02149534225464\n",
      "MSE train 1.8225811106256629 MSE test 2.435964265741722\n",
      "Epoch 180 / 200 loss: 87.82092046737671\n",
      "MSE train 1.8184302211133034 MSE test 2.4318813066765093\n",
      "Epoch 181 / 200 loss: 87.62442433834076\n",
      "MSE train 1.8143435693441994 MSE test 2.428025618476907\n",
      "Epoch 182 / 200 loss: 87.42512094974518\n",
      "MSE train 1.8103962212453946 MSE test 2.4242820760352686\n",
      "Epoch 183 / 200 loss: 87.22895610332489\n",
      "MSE train 1.806595384918893 MSE test 2.420579168580896\n",
      "Epoch 184 / 200 loss: 87.03966987133026\n",
      "MSE train 1.802876352208216 MSE test 2.4169412804479937\n",
      "Epoch 185 / 200 loss: 86.85724425315857\n",
      "MSE train 1.7990651841451337 MSE test 2.4132632018148015\n",
      "Epoch 186 / 200 loss: 86.67842626571655\n",
      "MSE train 1.7952548746953219 MSE test 2.4095529836101868\n",
      "Epoch 187 / 200 loss: 86.49519193172455\n",
      "MSE train 1.7914519425341473 MSE test 2.405909751479299\n",
      "Epoch 188 / 200 loss: 86.31197798252106\n",
      "MSE train 1.7876606143607892 MSE test 2.402334296353873\n",
      "Epoch 189 / 200 loss: 86.12920463085175\n",
      "MSE train 1.7838286728323691 MSE test 2.398801008739348\n",
      "Epoch 190 / 200 loss: 85.9470419883728\n",
      "MSE train 1.7798753173959125 MSE test 2.395260753681457\n",
      "Epoch 191 / 200 loss: 85.76257860660553\n",
      "MSE train 1.775802700749563 MSE test 2.3916857047110356\n",
      "Epoch 192 / 200 loss: 85.57190418243408\n",
      "MSE train 1.7719335389708017 MSE test 2.3881536910482772\n",
      "Epoch 193 / 200 loss: 85.37639939785004\n",
      "MSE train 1.7682935075429365 MSE test 2.384736347982643\n",
      "Epoch 194 / 200 loss: 85.19133079051971\n",
      "MSE train 1.764786257935784 MSE test 2.3813739828190728\n",
      "Epoch 195 / 200 loss: 85.01661503314972\n",
      "MSE train 1.7613525884386674 MSE test 2.378065571139166\n",
      "Epoch 196 / 200 loss: 84.84802222251892\n",
      "MSE train 1.7578890192852503 MSE test 2.374769519637896\n",
      "Epoch 197 / 200 loss: 84.68264853954315\n",
      "MSE train 1.7544143771014011 MSE test 2.371464478970109\n",
      "Epoch 198 / 200 loss: 84.51609468460083\n",
      "MSE train 1.7508939492467368 MSE test 2.3681204279162325\n",
      "Epoch 199 / 200 loss: 84.34940922260284\n"
     ]
    }
   ],
   "source": [
    "# running the model for a 200 epochs taking 100 users in batches\n",
    "# total improvement is printed out after each epoch\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    \n",
    "    for i in range(int(tot_users/batch_size)):\n",
    "        epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "        \n",
    "    output_train = sess.run(output_layer,\\\n",
    "               feed_dict={input_layer:X_train})\n",
    "    output_test = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:X_test})\n",
    "        \n",
    "    print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))      \n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[99,:]\n",
    "#get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0811718 ,  0.9921726 , -0.5388974 , ...,  0.14187828,\n",
       "         0.72228765, -0.65306234]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_user_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
